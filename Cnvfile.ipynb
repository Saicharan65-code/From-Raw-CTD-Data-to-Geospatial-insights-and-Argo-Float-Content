{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a000aa9-b64b-4ae7-aa64-206242bc12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import rasterio\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def make_unique(columns):\n",
    "    counts = Counter()\n",
    "    unique_cols = []\n",
    "    for col in columns:\n",
    "        counts[col] += 1\n",
    "        if counts[col] > 1:\n",
    "            unique_cols.append(f\"{col}_{counts[col]}\")\n",
    "        else:\n",
    "            unique_cols.append(col)\n",
    "    return unique_cols\n",
    "def extract_base_name(col):\n",
    "    col_lower = col.lower()\n",
    "    if col_lower.startswith(\"flag\"):\n",
    "        return \"flag\"\n",
    "\n",
    "    return col.split(':')[0].strip()\n",
    "def merge_duplicate_columns(data_df):\n",
    "    from collections import defaultdict\n",
    "    grouped = defaultdict(list)\n",
    "    for col in data_df.columns:\n",
    "        base = extract_base_name(col)\n",
    "        grouped[base].append(col)\n",
    "    for base, cols in grouped.items():\n",
    "        if len(cols) > 1:\n",
    "            data_df[base] = data_df[cols].mean(axis=1)\n",
    "            data_df.drop(columns=cols, inplace=True)\n",
    "        elif base not in data_df.columns:\n",
    "            data_df[base] = data_df[cols[0]]\n",
    "    return data_df\n",
    "def clean_lat_long_pair(lat_raw, lon_raw):\n",
    "    def extract_digits_and_dir(value):\n",
    "        value = value.replace(',', '.').replace('v', '.')\n",
    "        cleaned = re.sub(r'(?<=\\d)[ .]+(?=\\d)', '', value)\n",
    "        cleaned = re.sub(r'[ .]', '', cleaned)\n",
    "        match = re.match(r'^(\\d+)([NSEWnsew]?)$', cleaned)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Invalid coordinate format: {value}\")\n",
    "        digits, direction = match.groups()\n",
    "        direction = direction.upper() if direction else None\n",
    "        return digits, direction\n",
    "    lat_digits, lat_dir = extract_digits_and_dir(lat_raw)\n",
    "    lon_digits, lon_dir = extract_digits_and_dir(lon_raw)\n",
    "    if lat_dir not in ['N', 'S']:\n",
    "        lat_dir = 'N'\n",
    "    if lon_dir not in ['E', 'W']:\n",
    "        lon_dir = 'E'\n",
    "    if lat_dir in ['E', 'W'] and lon_dir in ['E', 'W']:\n",
    "        print(\" Latitude has direction like longitude — correcting to N\")\n",
    "        lat_dir = 'N'\n",
    "    elif lat_dir in ['N', 'S'] and lon_dir in ['N', 'S']:\n",
    "        print(\" Longitude has direction like latitude — correcting to E\")\n",
    "        lon_dir = 'E'\n",
    "    return lat_digits + lat_dir, lon_digits + lon_dir\n",
    "def parse_latitude(lat_str):\n",
    "        length = len(lat_str)\n",
    "        meta = lat_str[-1].upper()\n",
    "        digits = lat_str[:-1]\n",
    "        deg = min = sec = 0\n",
    "        if length == 2 + 1:\n",
    "            if lat_str[0] == '0':\n",
    "                deg = int(digits[:2])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "        elif length == 4 + 1:\n",
    "            if lat_str[0] == '0':\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = 0\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "        elif length == 5 + 1:\n",
    "            if digits[0] in ['0', '1', '2']:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:5])\n",
    "            else:\n",
    "                deg = int(digits[0])\n",
    "                min = int(digits[1:3])\n",
    "                sec = int(digits[3:5])\n",
    "        elif length == 6 + 1:\n",
    "            if digits[0] in ['0', '1', '2']:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:6])\n",
    "            else:\n",
    "                deg = int(digits[:1])\n",
    "                min = int(digits[1:3])\n",
    "                sec = int(digits[3:6])\n",
    "        elif length == 7 + 1:\n",
    "            if digits[0] in ['0', '1', '2']:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:7])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:7])\n",
    "        else:\n",
    "            raise ValueError(\"unsupported\")\n",
    "        if sec > 59:\n",
    "            if sec <=599:\n",
    "                sec = sec/10\n",
    "            else:\n",
    "                sec = sec/100\n",
    "        decimal = deg + (min/60) + (sec/3600)\n",
    "        if meta in ['S', 'W']:\n",
    "            decimal *=-1\n",
    "        return round(decimal, 4)\n",
    "def parse_longitude(lon_str):\n",
    "        length = len(lon_str)\n",
    "        meta = lon_str[-1].upper()\n",
    "        digits = lon_str[:-1]\n",
    "        deg = min = sec = 0\n",
    "        if length == 2 + 1:\n",
    "            if lon_str[0] == '0':\n",
    "                deg = int(digits[:2])\n",
    "            else:\n",
    "                deg = int(digits[:2])   \n",
    "        elif length == 4 + 1:\n",
    "            if lon_str[0] == '0':\n",
    "                deg = int(digits[:3])\n",
    "                min = int(digits[3:4])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "        elif length == 5 + 1:\n",
    "            if lon_str[0] == '0':\n",
    "                deg = int(digits[:3])\n",
    "                min = int(digits[3:5])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:5])\n",
    "        elif length == 6 + 1:\n",
    "            if lon_str[0] == '0':\n",
    "                deg = int(digits[:3])\n",
    "                min = int(digits[3:5])\n",
    "                sec = int(digits[5:6])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:6])\n",
    "        elif length == 7 + 1:\n",
    "            if lon_str[0] == '0':\n",
    "                deg = int(digits[:3])\n",
    "                min = int(digits[3:5])\n",
    "                sec = int(digits[5:7])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:7])\n",
    "        elif length == 8 + 1:\n",
    "            if lon_str[0] == '0':\n",
    "                deg = int(digits[:3])\n",
    "                min = int(digits[3:5])\n",
    "                sec = int(digits[5:8])\n",
    "            else:\n",
    "                deg = int(digits[:2])\n",
    "                min = int(digits[2:4])\n",
    "                sec = int(digits[4:8])\n",
    "        else:\n",
    "            raise ValueError(\"unsupported\")\n",
    "        if sec > 59:\n",
    "            if sec <=599:\n",
    "                sec = sec/10\n",
    "            else:\n",
    "                sec = sec/100\n",
    "        decimal = deg + (min/60) + (sec/3600)\n",
    "        if meta in ['S', 'W']:\n",
    "            decimal *=-1\n",
    "        return round(decimal, 4)\n",
    "def parse_cnv(file_path, relative_path):\n",
    "    print(f\"Parsing {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    metadata = {}\n",
    "    parameters = []\n",
    "    data_started = False\n",
    "    data_lines = []\n",
    "    lat_raw = None\n",
    "    lon_raw = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"*END*\"):\n",
    "            data_started = True\n",
    "            continue\n",
    "        if not data_started:\n",
    "            if line.startswith(\"** Ship:\") or line.startswith(\"** Vessel:\"):\n",
    "                parts = line.split(\":\", 1)\n",
    "                metadata[\"Ship\"] = parts[1].strip() if len(parts) > 1 and parts[1].strip() else \"NAN\"\n",
    "            elif line.startswith(\"** Cruise:\"):\n",
    "                parts = line.split(\":\", 1)\n",
    "                metadata[\"Cruise ID\"] = parts[1].strip() if len(parts) > 1 and parts[1].strip() else \"NAN\"\n",
    "            elif line.startswith(\"** Station:\"):\n",
    "                parts = line.split(\":\", 1)\n",
    "                metadata[\"Station\"] = parts[1].strip() if len(parts) > 1 and parts[1].strip() else \"NA\"\n",
    "            elif line.startswith(\"** Latitude:\") or line.startswith(\"** LAT:\") or line.startswith(\"* NMEA Latitude\"):\n",
    "                lat_raw = line.split(\"=\", 1)[1].strip() if \"=\" in line else line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"** Longitude:\") or line.startswith(\"** LON:\") or line.startswith(\"* NMEA Longitude\"):\n",
    "                lon_raw = line.split(\"=\", 1)[1].strip() if \"=\" in line else line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"# start_time\"):\n",
    "                parts = line.split(\"=\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    metadata[\"Start Time\"] = parts[1].strip()\n",
    "            elif line.startswith(\"# name\"):\n",
    "                parts = line.split(\"=\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    parameters.append(parts[1].strip())    \n",
    "        elif line and not line.startswith(\"#\"):\n",
    "            data_lines.append(line)\n",
    "    if lat_raw and lon_raw:\n",
    "        try:\n",
    "            lat_clean, lon_clean = clean_lat_long_pair(lat_raw, lon_raw)\n",
    "            metadata[\"Latitude\"] = parse_latitude(lat_clean)\n",
    "            metadata[\"Longitude\"] = parse_longitude(lon_clean)\n",
    "            metadata[\"LAT\"] = parse_latitude(lat_clean)\n",
    "            metadata[\"LON\"] = parse_longitude(lon_clean)\n",
    "            metadata[\"NMEA Latitude\"] = parse_latitude(lat_clean)\n",
    "            metadata[\"NMEA Longitude\"] = parse_longitude(lon_clean)\n",
    "        except Exception as e:\n",
    "            metadata[\"Latitude\"] = \"NA\"\n",
    "            metadata[\"Longitude\"] = \"NA\"\n",
    "            metadata[\"LAT\"] = \"NA\"\n",
    "            metadata[\"LON\"] = \"NA\"\n",
    "            metadata[\"NMEA Latitude\"] = \"NA\"\n",
    "            metadata[\"NMEA Longitude\"] = \"NA\"\n",
    "            print(f\"Unsupported lat/lon in: {file_path}: {e}\")\n",
    "    if not parameters:\n",
    "        raise ValueError(f\"No parameters found in {file_path}\")\n",
    "    column_names = make_unique([param.split(':')[0].strip() for param in parameters])\n",
    "    data = []\n",
    "    for line in data_lines:\n",
    "        try:\n",
    "            values = list(map(float, line.split()) )\n",
    "            data.append(values)\n",
    "        except ValueError:\n",
    "            continue  \n",
    "    data_df = pd.DataFrame(data, columns=column_names)\n",
    "    data_df = merge_duplicate_columns(data_df)\n",
    "    column_groups = {\n",
    "        \"c0mS/cm\": [\"C0S/m\"],\n",
    "        \"Sal00\": [\"sal00\"]\n",
    "    }\n",
    "    \n",
    "    for unified_col, variants in column_groups.items():\n",
    "        present = [col for col in data_df.columns if col in variants]\n",
    "        if present:\n",
    "            data_df[unified_col] = data_df[present].bfill(axis=1).iloc[:, 0]\n",
    "            to_drop = [col for col in present if col != unified_col]\n",
    "            data_df.drop(columns=to_drop, inplace=True)\n",
    "    raw_names = ['prDM', 't090C', 'Sal00', 'sbeox0ML/L', 'depSM', 'svCM', 'specc', 'xmiss', 'bat', 'scan', 'flECO-AFL', 'sbeox0PS', 'par', 'density00', 'sigma-é00', 'flag', 'oxsatML/L', 'c0mS/cm', 'potemp090C', 'potemp068C', 'sigma-t00']\n",
    "    keep_columns = [col for col in data_df.columns if col in raw_names]\n",
    "    clean_str = lambda s: str(s).replace(\" \", \"\").replace(\":\",\"-\") if s is not None else \"NA\"\n",
    "    trace_id_parts = [\n",
    "        clean_str(metadata.get(\"Cruise ID\")),\n",
    "        clean_str(metadata.get(\"Station\")),\n",
    "        clean_str(metadata.get(\"Latitude\")),\n",
    "        clean_str(metadata.get(\"Longitude\")),\n",
    "        clean_str(metadata.get(\"Start Time\")),\n",
    "    ]\n",
    "    trace_id = \"_\".join(trace_id_parts)\n",
    "    metadata[\"TraceID\"] = trace_id\n",
    "    data_df[\"TraceID\"] = trace_id\n",
    "    keep_columns.append(\"TraceID\")\n",
    "    data_df = data_df[keep_columns]\n",
    "    keys = [\"Ship\", \"Cruise ID\", \"Station\", \"Latitude\", \"Longitude\", \"Start Time\"]\n",
    "    meta_values = [metadata.get(k, None) for k in keys]\n",
    "    meta_df = pd.DataFrame([meta_values], columns=keys)\n",
    "    meta_df[\"Parameters\"] = \", \".join(column_names)\n",
    "    meta_df[\"TraceID\"] = trace_id\n",
    "    meta_df[\"folderpath_filename\"] = relative_path\n",
    "    data_df[\"folderpath_filename\"] = relative_path\n",
    "    meta_cols = meta_df.columns.tolist()\n",
    "    data_cols = data_df.columns.tolist()\n",
    "    meta_cols.insert(meta_cols.index(\"TraceID\") + 1, meta_cols.pop(meta_cols.index(\"folderpath_filename\")))\n",
    "    data_cols.insert(data_cols.index(\"TraceID\") + 1, data_cols.pop(data_cols.index(\"folderpath_filename\")))\n",
    "    meta_df = meta_df[meta_cols]\n",
    "    data_df = data_df[data_cols]\n",
    "    return meta_df, data_df\n",
    "def preprocess_cnv_folder(folder_path, output_meta_csv='CNV_table.csv', output_data_csv='CNVd_table.csv'):\n",
    "    all_metadata = []\n",
    "    all_data = []\n",
    "    all_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        cnv_files = [os.path.join(dirpath, f) for f in filenames if f.lower().endswith(\".cnv\")]\n",
    "        all_files.extend(cnv_files)\n",
    "    print(f\"Found {len(all_files)} CNV files...\")\n",
    "    for file_path in all_files:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            relative_path = os.path.relpath(file_path, folder_path).replace(\"\\\\\", \"/\")\n",
    "            meta_df, df = parse_cnv(file_path, relative_path)\n",
    "            all_metadata.append(meta_df.to_dict(orient='records')[0])\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    if all_data:\n",
    "        data_table = pd.concat(all_data, ignore_index=True)\n",
    "        data_table.to_csv(output_data_csv, index=False, na_rep='NaN')\n",
    "    if all_metadata:\n",
    "        meta_table = pd.DataFrame(all_metadata)\n",
    "        meta_table.to_csv(output_meta_csv, index=False, na_rep='NaN')\n",
    "    print(f\"\\n Metadata saved to: {output_meta_csv}\")\n",
    "    print(f\" Data table saved to: {output_data_csv}\")\n",
    "\n",
    "# ========== Run Main ==========\n",
    "if __name__ == \"__main__\":\n",
    "    folder = r\"C:\\Users\\aishwarya\\OneDrive\\Desktop\\257A-290-ST\" \n",
    "    output_meta_csv = 'CNV_table.csv'\n",
    "    output_data_csv = 'CNVd_table.csv'\n",
    "    \n",
    "    preprocess_cnv_folder(folder, output_meta_csv, output_data_csv)\n",
    "    \n",
    "    try:\n",
    "        meta_table = pd.read_csv(output_meta_csv)\n",
    "        excel_path = r\"C:\\Users\\aishwarya\\Downloads\\Lat_Long.xlsx\"\n",
    "        excel_df = pd.read_excel(excel_path)\n",
    "        if 'TraceID' not in meta_table.columns:\n",
    "            raise ValueError(\"TraceID column missing in meta_table\")\n",
    "        original_traceids = meta_table['TraceID'].copy()\n",
    "\n",
    "        def extract_clean_station(value):\n",
    "            filename = os.path.basename(value)          \n",
    "            name = os.path.splitext(filename)[0]        \n",
    "            name = name.replace(\"_\", \"\")                \n",
    "            return name.strip().upper()\n",
    "         \n",
    "        meta_table[\"clean_station\"] = meta_table[\"folderpath_filename\"].apply(extract_clean_station)\\\n",
    "            .str.replace(\".cnv\", \"\", case=False)\\\n",
    "            .str.strip().str.upper().str.replace(\" \", \"\")\n",
    "        excel_df[\"clean_station\"] = excel_df[\"Stations\"]\\\n",
    "            .str.replace(\".hex\", \"\", case=False)\\\n",
    "            .str.replace(\"_\", \"\")\\\n",
    "            .str.strip().str.upper().str.replace(\" \", \"\")\n",
    "        merged = meta_table.merge(excel_df[[\"clean_station\", \"Lat\", \"Long\"]],\n",
    "                                 on=\"clean_station\", how=\"left\", suffixes=('', '_excel'))\n",
    "        merged[\"Latitude\"] = merged[\"Latitude\"].fillna(merged[\"Lat\"])\n",
    "        merged[\"Longitude\"] = merged[\"Longitude\"].fillna(merged[\"Long\"])\n",
    "        merged[\"Latitude\"] = pd.to_numeric(merged[\"Latitude\"], errors=\"coerce\")\n",
    "        merged[\"Longitude\"] = pd.to_numeric(merged[\"Longitude\"], errors=\"coerce\")\n",
    "        merged[\"Latitude\"] = merged[\"Latitude\"].map(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"NaN\")\n",
    "        merged[\"Longitude\"] = merged[\"Longitude\"].map(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"NaN\")\n",
    "        def clean_str(s):\n",
    "            return str(s).replace(\" \", \"\").replace(\":\", \"-\") if pd.notna(s) else \"NA\"\n",
    "\n",
    "        merged[\"TraceID\"] = merged.apply(lambda row: \"_\".join([\n",
    "            clean_str(row.get(\"Cruise ID\")),\n",
    "            clean_str(row.get(\"Station\")),\n",
    "            clean_str(row.get(\"Latitude\")),\n",
    "            clean_str(row.get(\"Longitude\")),\n",
    "            clean_str(row.get(\"Start Time\"))\n",
    "        ]), axis=1)\n",
    "\n",
    "        traceid_mapping = pd.DataFrame({\n",
    "            'old_TraceID': original_traceids,\n",
    "            'new_TraceID': merged['TraceID']\n",
    "        }).drop_duplicates()\n",
    "\n",
    "        merged.drop(columns=[\"clean_station\", \"Lat\", \"Long\"], inplace=True)\n",
    "        merged.to_csv(output_meta_csv, index=False, na_rep='NaN')\n",
    "        print(f\" Updated meta with Excel mapping + new TraceID → saved to: {output_meta_csv}\")\n",
    "        data_table = pd.read_csv(output_data_csv)\n",
    "        \n",
    "        if 'TraceID' not in data_table.columns:\n",
    "            raise ValueError(\"TraceID column missing in data_table\")\n",
    "\n",
    "        data_table = data_table.merge(traceid_mapping, \n",
    "                                    left_on='TraceID', \n",
    "                                    right_on='old_TraceID', \n",
    "                                    how='left')\n",
    "        data_table['TraceID'] = data_table['new_TraceID'].fillna(data_table['TraceID'])\n",
    "        data_table.drop(columns=['old_TraceID', 'new_TraceID'], inplace=True)\n",
    "        \n",
    "        data_table.to_csv(output_data_csv, index=False, na_rep='NaN')\n",
    "        print(f\" Updated TraceIDs in datatable → saved to: {output_data_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to map Excel coordinates or update TraceID: {str(e)}\")\n",
    "        if 'meta_table' in locals():\n",
    "            print(\"Meta table columns:\", meta_table.columns.tolist())\n",
    "        if 'merged' in locals():\n",
    "            print(\"Merged table columns:\", merged.columns.tolist())\n",
    "#--------QC----------\n",
    "print(\"\\n Running QC on extracted data...\")\n",
    "\n",
    "meta = pd.read_csv('CNV_table.csv')\n",
    "data = pd.read_csv('CNVd_table.csv')\n",
    "raster = rasterio.open(r\"C:\\Users\\aishwarya\\Downloads\\ETOPO1_Bed_g_geotiff\\ETOPO1_Bed_g_geotiff.tif\")\n",
    "bathymetry = raster.read(1)\n",
    "\n",
    "def is_at_sea(lat, lon):\n",
    "    try:\n",
    "        row, col = raster.index(lon, lat)\n",
    "        return bathymetry[row, col] < 0\n",
    "    except:\n",
    "        return False\n",
    "# === Profile Envelope QC Range for TEMP (GTSPP)\n",
    "TEMP_PROFILE_ENVELOPE = [\n",
    "    {\"min_depth\": 0, \"max_depth\": 1100, \"min_value\": -2.0, \"max_value\": 40.0},\n",
    "    {\"min_depth\": 1100, \"max_depth\": 3000, \"min_value\": -1.5, \"max_value\": 18.0},\n",
    "]\n",
    "\n",
    "def get_profile_envelope(depth, envelope_table):\n",
    "    for layer in envelope_table:\n",
    "        if layer[\"min_depth\"] <= depth < layer[\"max_depth\"]:\n",
    "            return layer[\"min_value\"], layer[\"max_value\"]\n",
    "    return None, None\n",
    "def profile_envelope_qc(df, depth_col='depSM', param_col='t090C', envelope_table=TEMP_PROFILE_ENVELOPE):\n",
    "    flags = []\n",
    "    for _, row in df.iterrows():\n",
    "        depth = row.get(depth_col)\n",
    "        value = row.get(param_col)\n",
    "\n",
    "        if pd.isna(depth) or pd.isna(value):\n",
    "            flags.append(9) \n",
    "            continue\n",
    "\n",
    "        min_val, max_val = get_profile_envelope(depth, envelope_table)\n",
    "        if min_val is None:\n",
    "            flags.append(9)  \n",
    "        elif min_val <= value <= max_val:\n",
    "            flags.append(1) \n",
    "        else:\n",
    "            flags.append(4)  \n",
    "    return pd.Series(flags, name=f'{param_col}_PROFILE_QC')\n",
    "\n",
    "# === QC 1: Valid datetime\n",
    "meta['datetime'] = pd.to_datetime(meta['Start Time'], errors='coerce')\n",
    "meta['DATE_QC'] = pd.to_datetime(meta['Start Time'], errors='coerce').dt.year.gt(1997).map({True: 1, False: 4})\n",
    "\n",
    "# === QC 2: Valid position\n",
    "valid_lat = meta['Latitude'].between(-40, 30)\n",
    "valid_lon = meta['Longitude'].between(20, 160)\n",
    "meta['POS_QC'] = ((valid_lat & valid_lon)).map({True: 1, False: 4})\n",
    "\n",
    "# === QC 3: Location at Sea\n",
    "print(\" Checking location at sea...\")\n",
    "meta['SEA_QC'] = meta.apply(lambda row: 1 if is_at_sea(row['Latitude'], row['Longitude']) else 4, axis=1)\n",
    "\n",
    "# === Combine all three station-level QC tests\n",
    "meta_valid = meta[(meta['DATE_QC'] == 1) & (meta['POS_QC'] == 1) & (meta['SEA_QC'] == 1)]\n",
    "\n",
    "# === Filter data to only valid profiles\n",
    "valid_trace_ids = meta_valid['TraceID'].tolist()\n",
    "#data = data[data['TraceID'].isin(valid_trace_ids)]\n",
    "\n",
    "# === Gradient and Spike QC Functions\n",
    "def gradient_test(series, threshold):\n",
    "    result = (series - (series.shift(-1) + series.shift(1)) / 2).abs() <= threshold\n",
    "    return result.map({True: 1, False: 4})\n",
    "\n",
    "def spike_test(series, threshold):\n",
    "    part1 = (series - (series.shift(-1) + series.shift(1)) / 2).abs()\n",
    "    part2 = ((series.shift(-1) - series.shift(1)) / 2).abs()\n",
    "    result = (part1 - part2) <= threshold\n",
    "    return result.map({True: 1, False: 4})\n",
    "\n",
    "# === QC 4–6: Variable-level QC for TEMP and PSAL\n",
    "if 't090C' in data.columns:\n",
    "    data['TEMP_QC'] = data['t090C'].between(-2, 40).map({True: 1, False: 4})\n",
    "    data['TEMP_GRAD_QC'] = gradient_test(data['t090C'], 10.0)\n",
    "    data['TEMP_SPIKE_QC'] = spike_test(data['t090C'], 2.0)\n",
    "\n",
    "if 'Sal00' in data.columns:\n",
    "    data['PSAL_QC'] = data['Sal00'].between(0, 41).map({True: 1, False: 4})\n",
    "    data['PSAL_GRAD_QC'] = gradient_test(data['Sal00'], 5.0)\n",
    "    data['PSAL_SPIKE_QC'] = spike_test(data['Sal00'], 0.3)\n",
    "# === QC 7: Profile Envelope Test\n",
    "if 'depSM' in data.columns and 't090C' in data.columns:\n",
    "    data['TEMP_PROFILE_QC'] = profile_envelope_qc(data, depth_col='depSM', param_col='t090C')\n",
    "\n",
    "\n",
    "meta.to_csv(\"meta1.csv\", index=False,na_rep='NaN')\n",
    "data.to_csv(\"data1.csv\", index=False,na_rep='NaN')\n",
    "\n",
    "print(\" QC complete. Saved meta1.csv and data1.csv \")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
