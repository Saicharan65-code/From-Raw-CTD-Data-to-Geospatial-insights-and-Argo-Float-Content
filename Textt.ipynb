{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24e28f8-fc7d-46ea-996a-477ea7431fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Found 11 text files in: C:\\Users\\aishwarya\\Videos\\Captures\\Desktop\\303\n",
      " Processing: SN031ctd01.TXT\n",
      " Processing: SN031ctd02.txt\n",
      " Processing: SN031ctd04.txt\n",
      " Processing: SN031ctd07.txt\n",
      " Processing: SN031ctd08.TXT\n",
      " Processing: SN031ctd09.TXT\n",
      " No header found. Trying fixed-column parsing for: SN031ctd09.TXT\n",
      " Processing: SN031ctd10.TXT\n",
      " Processing: SN031ctd3.txt\n",
      " Processing: SN031ctd5001.txt\n",
      " Processing: SN031ctd5002.txt\n",
      " Processing: SN031ctd6.txt\n",
      " Data saved to: textnd.csv\n",
      " Metadata saved to: textn.csv\n",
      "\n",
      " Running QC on extracted data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishwarya\\AppData\\Local\\Temp\\ipykernel_17232\\3146213897.py:258: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('textnd.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checking location at sea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishwarya\\anaconda3\\Lib\\site-packages\\rasterio\\transform.py:410: RuntimeWarning: invalid value encountered in cast\n",
      "  new_rows = np.floor(new_rows).astype(dtype=\"int32\")\n",
      "C:\\Users\\aishwarya\\anaconda3\\Lib\\site-packages\\rasterio\\transform.py:411: RuntimeWarning: invalid value encountered in cast\n",
      "  new_cols = np.floor(new_cols).astype(dtype=\"int32\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QC complete. Saved textnQC.csv and textndQC.csv \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import rasterio\n",
    "\n",
    "def make_unique(columns):\n",
    "    counts = Counter()\n",
    "    result = []\n",
    "    for col in columns:\n",
    "        counts[col] += 1\n",
    "        result.append(f\"{col}_{counts[col]}\" if counts[col] > 1 else col)\n",
    "    return result\n",
    "\n",
    "def parse_txt_combined(file_path, relative_path=\"\"):\n",
    "    with open(file_path, 'r', encoding='latin1') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    metadata = {\n",
    "        \"Ship\": float('nan'),\n",
    "        \"Cruise ID\": float('nan'),\n",
    "        \"Station\": float('nan'),\n",
    "        \"Latitude\": float('nan'),\n",
    "        \"Longitude\": float('nan'),\n",
    "        \"Start Time\": float('nan'),\n",
    "    }\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"** Ship:\"):\n",
    "            metadata[\"Ship\"] = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"** Cruise:\"):\n",
    "            metadata[\"Cruise ID\"] = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"** Station:\"):\n",
    "            metadata[\"Station\"] = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"** Latitude:\"):\n",
    "            metadata[\"Latitude\"] = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"** Longitude:\"):\n",
    "            metadata[\"Longitude\"] = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"# start_time\"):\n",
    "            parts = line.split(\"=\", 1)\n",
    "            if len(parts) > 1:\n",
    "                metadata[\"Start Time\"] = parts[1].strip()\n",
    "\n",
    "    def safe_str(v):\n",
    "        return \"NaN\" if pd.isna(v) else str(v)\n",
    "\n",
    "    metadata[\"TraceID\"] = f\"{safe_str(metadata['Cruise ID'])}_{safe_str(metadata['Station'])}_{safe_str(metadata['Latitude'])}_{safe_str(metadata['Longitude'])}_{safe_str(metadata['Start Time'])}\"\n",
    "    metadata[\"folderpath_filename\"] = relative_path\n",
    "\n",
    "    # === Type 1: Standard headers like Pressure, Depth, etc.\n",
    "    header_keywords = ['Pressure', 'Depth', 'Temperature', 'Theta']\n",
    "    header_line = next((line for line in lines if any(k in line for k in header_keywords)), None)\n",
    "\n",
    "    if header_line:\n",
    "        raw_columns = header_line.split()\n",
    "        columns = make_unique(raw_columns)\n",
    "        start_index = lines.index(header_line) + 1\n",
    "        data_rows = []\n",
    "\n",
    "        for line in lines[start_index:]:\n",
    "            if any(k in line for k in header_keywords):\n",
    "                continue\n",
    "            if not any(c.isdigit() for c in line):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < len(columns):\n",
    "                parts += [None] * (len(columns) - len(parts))\n",
    "            elif len(parts) > len(columns):\n",
    "                parts = parts[:len(columns)]\n",
    "            data_rows.append(parts)\n",
    "\n",
    "        if not data_rows:\n",
    "            raise ValueError(\"No valid data rows\")\n",
    "\n",
    "        df = pd.DataFrame(data_rows, columns=columns)\n",
    "        df.columns = make_unique(df.columns)\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    else:\n",
    "        # === Type 2: Block headers like \"Press Temp Cond ...\"\n",
    "        block_headers = ['press', 'temp', 'cond', 'sal', 'o2', 'ph', 'chl', 'tr%', 'memory']\n",
    "        data_blocks = []\n",
    "        current_columns = []\n",
    "        current_data = []\n",
    "        pre_header_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"Keyb.Cmd\" in line or \"<ESC>\" in line:\n",
    "                continue\n",
    "            if sum(c.isalnum() for c in line) < len(line) * 0.3:\n",
    "                continue\n",
    "            if not all(32 <= ord(c) <= 126 or c.isspace() for c in line):\n",
    "                continue\n",
    "        \n",
    "            tokens = line.split()\n",
    "            token_match = all(\n",
    "                any(c.isalpha() for c in t) and all(32 <= ord(c) <= 126 for c in t)\n",
    "                for t in tokens\n",
    "            )\n",
    "            header_match = any(k in line.lower() for k in block_headers)\n",
    "            if token_match and header_match:\n",
    "                if not all(32 <= ord(c) <= 126 or c.isspace() for c in line):\n",
    "                    continue\n",
    "\n",
    "                if current_columns and current_data:\n",
    "                    df = pd.DataFrame(current_data, columns=current_columns)\n",
    "                    df.columns = make_unique(df.columns)\n",
    "                    for col in df.columns:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    data_blocks.append(df)\n",
    "                    current_data = []\n",
    "                current_columns = make_unique(tokens)\n",
    "                for row in pre_header_data:\n",
    "                    if len(row) < len(current_columns):\n",
    "                        row += [None] * (len(current_columns) - len(row))\n",
    "                    elif len(row) > len(current_columns):\n",
    "                        row = row[:len(current_columns)]\n",
    "                    current_data.append(row)\n",
    "                pre_header_data = []\n",
    "                continue\n",
    "            if not current_columns:\n",
    "                if any(c.isdigit() for c in line):\n",
    "                    pre_header_data.append(line.split())\n",
    "                continue\n",
    "            if sum(c.isalnum() for c in line) < len(line) * 0.3:\n",
    "                continue\n",
    "            if not all(32 <= ord(c) <= 126 or c.isspace() for c in line):\n",
    "                continue\n",
    "        \n",
    "            # === Regular data row ===\n",
    "            parts = line.split()\n",
    "            if len(parts) < len(current_columns):\n",
    "                parts += [None] * (len(current_columns) - len(parts))\n",
    "            elif len(parts) > len(current_columns):\n",
    "                parts = parts[:len(current_columns)]\n",
    "            current_data.append(parts)\n",
    "        if current_columns and current_data:\n",
    "            df = pd.DataFrame(current_data, columns=current_columns)\n",
    "            df.columns = make_unique(df.columns)\n",
    "            for col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            data_blocks.append(df)\n",
    "\n",
    "        # === Fallback: no headers at all ===\n",
    "        if not data_blocks:\n",
    "            print(f\" No header found. Trying fixed-column parsing for: {os.path.basename(file_path)}\")\n",
    "        \n",
    "            data_rows = []\n",
    "        \n",
    "            for line in lines:\n",
    "                if not any(c.isdigit() for c in line):\n",
    "                    continue\n",
    "                tokens = line.split()\n",
    "                if len(tokens) < 8:\n",
    "                    continue\n",
    "                if not any(char.isdigit() for char in tokens[0]):\n",
    "                    tokens = tokens[1:]\n",
    "        \n",
    "                data_rows.append(tokens)\n",
    "        \n",
    "            if not data_rows:\n",
    "                raise ValueError(\"No valid data rows\")\n",
    "        \n",
    "            fallback_columns = [\n",
    "                \"Press\", \"Temp\", \"Cond\", \"Sal.\", \"O2%\", \"O2ppm\", \"pH\", \"Chl(a)\", \"Turb.Alt.\",  \"Tr%\",   \"Rho.\" ,\"memory\", \"flag\", \"time\"\n",
    "            ]\n",
    "        \n",
    "            max_cols = len(fallback_columns)\n",
    "            columns = fallback_columns\n",
    "            data_rows = [row[:max_cols] for row in data_rows]\n",
    "        \n",
    "            df = pd.DataFrame(data_rows, columns=columns)\n",
    "        \n",
    "            if \"time\" in df.columns:\n",
    "                time_col = df[\"time\"].astype(str)\n",
    "                df.drop(columns=[\"time\"], inplace=True)\n",
    "            else:\n",
    "                time_col = None\n",
    "        \n",
    "            for col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "            if time_col is not None:\n",
    "                df[\"time\"] = time_col\n",
    "\n",
    "\n",
    "        else:\n",
    "            df = pd.concat(data_blocks, ignore_index=True)\n",
    "    # === Normalize column variants ===\n",
    "    column_groups = {\n",
    "        \"prDM\": [\"Press\", \"Pressure\"],\n",
    "        \"t090C\": [\"Temp\", \"Temperature\"],\n",
    "        \"Sal00\": [\"Sal.\", \"Sal\", \"Salinity\"],\n",
    "        \"depSM\":[\"Depth\"],\n",
    "        \"c0mS/cm\":[\"Cond\"],\n",
    "        \"sbeox0ML/L\":[\"Oxygen\"],\n",
    "        \"sigma-t00\":[\"SigmaT\"],\n",
    "    }\n",
    "    \n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    \n",
    "    for unified_col, variants in column_groups.items():\n",
    "        present = [col for col in df.columns if col in variants]\n",
    "        if present:\n",
    "            df[unified_col] = df[present].bfill(axis=1).iloc[:, 0]\n",
    "            to_drop = [col for col in present if col != unified_col]\n",
    "            df.drop(columns=to_drop, inplace=True)\n",
    "    if \"Oxygen_2\" in df.columns:\n",
    "        df.drop(columns=[\"Oxygen_2\"], inplace=True)\n",
    "    df[\"TraceID\"] = metadata[\"TraceID\"]\n",
    "    df[\"folderpath_filename\"] = metadata[\"folderpath_filename\"]\n",
    "    metadata[\"parameters\"] = \", \".join(df.columns[:-2])\n",
    "\n",
    "    return pd.DataFrame([metadata]), df\n",
    "\n",
    "def preprocess_txt_folder(folder_path, output_meta_csv='textn.csv', output_data_csv='textnd.csv'):\n",
    "    all_metadata = []\n",
    "    all_data = []\n",
    "    all_files = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        txt_files = [os.path.join(dirpath, f) for f in filenames if f.lower().endswith('.txt')]\n",
    "        all_files.extend(txt_files)\n",
    "\n",
    "    print(f\"\\n Found {len(all_files)} text files in: {folder_path}\")\n",
    "\n",
    "    for file_path in all_files:\n",
    "        print(f\" Processing: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            rel_path = os.path.relpath(file_path, folder_path).replace(\"\\\\\", \"/\")\n",
    "            meta_df, df = parse_txt_combined(file_path, rel_path)\n",
    "\n",
    "            if df.columns.duplicated().any():\n",
    "                raise ValueError(f\"Duplicate columns found in {file_path}: {df.columns[df.columns.duplicated()].tolist()}\")\n",
    "\n",
    "            all_metadata.append(meta_df.to_dict(orient='records')[0])\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\" Error in {file_path}: {e}\")\n",
    "\n",
    "    if all_data:\n",
    "        data_table = pd.concat(all_data, ignore_index=True)\n",
    "        data_table.to_csv(output_data_csv, index=False, na_rep=\"NaN\")\n",
    "        print(f\" Data saved to: {output_data_csv}\")\n",
    "\n",
    "    if all_metadata:\n",
    "        meta_table = pd.DataFrame(all_metadata)\n",
    "        meta_table.to_csv(output_meta_csv, index=False, na_rep=\"NaN\")\n",
    "        print(f\" Metadata saved to: {output_meta_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder = r\"C:\\Users\\aishwarya\\Videos\\Captures\\Desktop\\303\"\n",
    "    preprocess_txt_folder(folder, output_meta_csv='textn.csv', output_data_csv='textnd.csv')\n",
    "#--------QC----------\n",
    "print(\"\\n Running QC on extracted data...\")\n",
    "\n",
    "meta = pd.read_csv('textn.csv')\n",
    "data = pd.read_csv('textnd.csv')\n",
    "raster = rasterio.open(r\"C:\\Users\\aishwarya\\Downloads\\ETOPO1_Bed_g_geotiff\\ETOPO1_Bed_g_geotiff.tif\")\n",
    "bathymetry = raster.read(1)\n",
    "\n",
    "def is_at_sea(lat, lon):\n",
    "    try:\n",
    "        row, col = raster.index(lon, lat)\n",
    "        return bathymetry[row, col] < 0\n",
    "    except:\n",
    "        return False\n",
    "# === Profile Envelope QC Range for TEMP (GTSPP)\n",
    "TEMP_PROFILE_ENVELOPE = [\n",
    "    {\"min_depth\": 0, \"max_depth\": 1100, \"min_value\": -2.0, \"max_value\": 40.0},\n",
    "    {\"min_depth\": 1100, \"max_depth\": 3000, \"min_value\": -1.5, \"max_value\": 18.0},\n",
    "]\n",
    "\n",
    "def get_profile_envelope(depth, envelope_table):\n",
    "    for layer in envelope_table:\n",
    "        if layer[\"min_depth\"] <= depth < layer[\"max_depth\"]:\n",
    "            return layer[\"min_value\"], layer[\"max_value\"]\n",
    "    return None, None\n",
    "def profile_envelope_qc(df, depth_col='depSM', param_col='t090C', envelope_table=TEMP_PROFILE_ENVELOPE):\n",
    "    flags = []\n",
    "    for _, row in df.iterrows():\n",
    "        depth = row.get(depth_col)\n",
    "        value = row.get(param_col)\n",
    "\n",
    "        if pd.isna(depth) or pd.isna(value):\n",
    "            flags.append(9)  # missing\n",
    "            continue\n",
    "\n",
    "        min_val, max_val = get_profile_envelope(depth, envelope_table)\n",
    "        if min_val is None:\n",
    "            flags.append(9)\n",
    "        elif min_val <= value <= max_val:\n",
    "            flags.append(1)\n",
    "        else:\n",
    "            flags.append(4) \n",
    "    return pd.Series(flags, name=f'{param_col}_PROFILE_QC')\n",
    "\n",
    "# === QC 1: Valid datetime\n",
    "meta['datetime'] = pd.to_datetime(meta['Start Time'], errors='coerce')\n",
    "meta['DATE_QC'] = pd.to_datetime(meta['Start Time'], errors='coerce').dt.year.gt(1997).map({True: 1, False: 4})\n",
    "\n",
    "# === QC 2: Valid position\n",
    "valid_lat = meta['Latitude'].between(-40, 30)\n",
    "valid_lon = meta['Longitude'].between(20, 160)\n",
    "meta['POS_QC'] = ((valid_lat & valid_lon)).map({True: 1, False: 4})\n",
    "\n",
    "# === QC 3: Location at Sea\n",
    "print(\" Checking location at sea...\")\n",
    "meta['SEA_QC'] = meta.apply(lambda row: 1 if is_at_sea(row['Latitude'], row['Longitude']) else 4, axis=1)\n",
    "\n",
    "# === Combine all three station-level QC tests\n",
    "meta_valid = meta[(meta['DATE_QC'] == 1) & (meta['POS_QC'] == 1) & (meta['SEA_QC'] == 1)]\n",
    "\n",
    "# === Filter data to only valid profiles\n",
    "valid_trace_ids = meta_valid['TraceID'].tolist()\n",
    "#data = data[data['TraceID'].isin(valid_trace_ids)]\n",
    "\n",
    "# === Gradient and Spike QC Functions\n",
    "def gradient_test(series, threshold):\n",
    "    result = (series - (series.shift(-1) + series.shift(1)) / 2).abs() <= threshold\n",
    "    return result.map({True: 1, False: 4})\n",
    "\n",
    "def spike_test(series, threshold):\n",
    "    part1 = (series - (series.shift(-1) + series.shift(1)) / 2).abs()\n",
    "    part2 = ((series.shift(-1) - series.shift(1)) / 2).abs()\n",
    "    result = (part1 - part2) <= threshold\n",
    "    return result.map({True: 1, False: 4})\n",
    "\n",
    "# === QC 4â€“6: Variable-level QC for TEMP and PSAL\n",
    "if 't090C' in data.columns:\n",
    "    data['TEMP_QC'] = data['t090C'].between(-2, 40).map({True: 1, False: 4})\n",
    "    data['TEMP_GRAD_QC'] = gradient_test(data['t090C'], 10.0)\n",
    "    data['TEMP_SPIKE_QC'] = spike_test(data['t090C'], 2.0)\n",
    "\n",
    "if 'Sal00' in data.columns:\n",
    "    data['PSAL_QC'] = data['Sal00'].between(0, 41).map({True: 1, False: 4})\n",
    "    data['PSAL_GRAD_QC'] = gradient_test(data['Sal00'], 5.0)\n",
    "    data['PSAL_SPIKE_QC'] = spike_test(data['Sal00'], 0.3)\n",
    "# === QC 7: Profile Envelope Test\n",
    "if 'depSM' in data.columns and 't090C' in data.columns:\n",
    "    data['TEMP_PROFILE_QC'] = profile_envelope_qc(data, depth_col='depSM', param_col='t090C')\n",
    "\n",
    "#meta.drop(columns=['at_sea'], inplace=True)\n",
    "meta.to_csv(\"texnQC.csv\", index=False,na_rep='NaN')\n",
    "data.to_csv(\"textndQC.csv\", index=False,na_rep='NaN')\n",
    "\n",
    "print(\" QC complete. Saved textnQC.csv and textndQC.csv \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f609a-895c-48d6-a523-0241627fb980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
