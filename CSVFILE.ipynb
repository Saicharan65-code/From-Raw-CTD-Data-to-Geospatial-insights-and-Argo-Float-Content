{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b252b9b6-c3c1-4963-9a98-ab7f12f65d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk103.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk104.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk105a.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk109.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk109.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk110.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk110a.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk113.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk113.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk115.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk116.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk118.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk119.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk121.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk137.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk140.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk141.csv\n",
      "âš  Format issue detected in C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk141.csv, attempting recovery with debug mode...\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk141.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk142.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk146.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk147a.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk147a.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\Sk147B.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishwarya\\AppData\\Local\\Temp\\ipykernel_13500\\3244335894.py:24: DtypeWarning: Columns (6,7,8,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk148.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk148.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk149D.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk175.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk179.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk179.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk189.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk194.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk194.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk195.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishwarya\\AppData\\Local\\Temp\\ipykernel_13500\\3244335894.py:24: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk200.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk207.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk207.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk212.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk219.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk219.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk220.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk227.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk63.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishwarya\\AppData\\Local\\Temp\\ipykernel_13500\\3244335894.py:24: DtypeWarning: Columns (6,7,8,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk70.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\sk70.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK124\\sk124-1_10stn_no_salinity.csv\n",
      "âš  Format issue detected in C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK124\\sk124-1_10stn_no_salinity.csv, attempting recovery with debug mode...\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK124\\sk124-1_10stn_no_salinity.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK124\\sk124.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK124\\sk124.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK138C\\sk138c-07.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK138C\\sk138c-10.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK138C\\sk138c-13.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\CTD1m\\SK138C\\sk138c-5-19.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\sk106\\sk106-interp.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\sk106\\sk106-interp.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\sk108\\sk108-interp.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\sk120\\sk120-interp.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\sk120\\sk120-interp.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\sk168\\sk168-interp.csv\n",
      "Reading: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\SK201\\sk201-interp.csv\n",
      "Debug processing: C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\\Interpolated-CTD\\SK201\\sk201-interp.csv\n",
      "Saved metadata to: 00_Moutput.csv (1878 rows)\n",
      "Saved data to: 00_doutput.csv (2144176 rows)\n",
      "\n",
      " Running QC on extracted data...\n",
      "ðŸ”¹ Checking location at sea...\n",
      " QC complete. Saved meta9.csv and data9.csv \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import rasterio\n",
    "def make_unique(columns):\n",
    "    counts = {}\n",
    "    result = []\n",
    "    for col in columns:\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            result.append(f\"{col}_{counts[col]}\")\n",
    "        else:\n",
    "            counts[col] = 1\n",
    "            result.append(col)\n",
    "    return result\n",
    "def is_number(val):\n",
    "    try:\n",
    "        float(val)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "def process_csv_same_headers_per_file(file_path):\n",
    "    print(f\"Reading: {file_path}\")\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    all_meta = []\n",
    "    data_blocks = []\n",
    "\n",
    "    meta_header = df.iloc[0].dropna().tolist()\n",
    "    meta_header = [x for x in meta_header if isinstance(x, str)]\n",
    "\n",
    "    i = 1\n",
    "    data_header = None\n",
    "\n",
    "    while i < len(df):\n",
    "        row_vals = df.iloc[i].dropna().tolist()\n",
    "\n",
    "        if len(row_vals) >= len(meta_header) // 2:\n",
    "            if len(row_vals) < len(meta_header):\n",
    "                row_vals += [''] * (len(meta_header) - len(row_vals))\n",
    "            metadata = dict(zip(meta_header, row_vals))\n",
    "            metadata['SourceFile'] = os.path.basename(file_path)\n",
    "            metadata['Folder'] = os.path.basename(os.path.dirname(file_path))\n",
    "            all_meta.append(metadata)\n",
    "            i += 1\n",
    "        elif data_header is None and len(row_vals) == 3 and all(not is_number(x) for x in row_vals):\n",
    "            data_header = row_vals\n",
    "            i += 1\n",
    "\n",
    "        elif data_header is not None and row_vals == data_header:\n",
    "            i += 1\n",
    "\n",
    "        elif data_header is not None and len(row_vals) == 3 and all(is_number(x) for x in row_vals):\n",
    "            data_rows = []\n",
    "            while i < len(df):\n",
    "                data_row = df.iloc[i].dropna().tolist()\n",
    "                if data_row == data_header:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                if len(data_row) == 3 and all(is_number(x) for x in data_row):\n",
    "                    data_rows.append(data_row)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            if data_rows:\n",
    "                if len(set(data_header)) != len(data_header):\n",
    "                    print(f\"Duplicate column names detected in {file_path}: {data_header}\")\n",
    "                    data_header = make_unique(data_header)\n",
    "                temp_df = pd.DataFrame(data_rows, columns=data_header)\n",
    "                temp_df['SourceFile'] = os.path.basename(file_path)\n",
    "                temp_df['Folder'] = os.path.basename(os.path.dirname(file_path))\n",
    "                data_blocks.append(temp_df)\n",
    "        else:\n",
    "            if data_header is None and len(row_vals) == 3 and all(is_number(x) for x in row_vals):\n",
    "                data_rows = []\n",
    "                col_count = 3\n",
    "                while i < len(df):\n",
    "                    data_row = df.iloc[i].dropna().tolist()\n",
    "                    if len(data_row) == col_count and all(is_number(x) for x in data_row):\n",
    "                        data_rows.append(data_row)\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if data_rows:\n",
    "                    dummy_header = [f\"Col{j+1}\" for j in range(col_count)]\n",
    "                    temp_df = pd.DataFrame(data_rows, columns=dummy_header)\n",
    "                    temp_df['SourceFile'] = os.path.basename(file_path)\n",
    "                    temp_df['Folder'] = os.path.basename(os.path.dirname(file_path))\n",
    "                    data_blocks.append(temp_df)\n",
    "            else:\n",
    "                i += 1\n",
    "    pairs = list(zip(all_meta, data_blocks))\n",
    "    return pairs\n",
    "def debug_process_csv(file_path):\n",
    "    print(f\"Debug processing: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        all_meta = []\n",
    "        data_blocks = []\n",
    "        meta_header = None\n",
    "        current_meta = None\n",
    "        last_meta = None\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if not line:\n",
    "                i += 1\n",
    "                continue\n",
    "            fields = [f.strip() for f in line.split(',') if f.strip()]\n",
    "            if meta_header is None and len(fields) >= 6 and all(not is_number(x) for x in fields):\n",
    "                meta_header = fields\n",
    "                i += 1\n",
    "                continue\n",
    "            if meta_header and len(fields) == len(meta_header):\n",
    "                current_meta = dict(zip(meta_header, fields))\n",
    "                current_meta['SourceFile'] = os.path.basename(file_path)\n",
    "                current_meta['Folder'] = os.path.basename(os.path.dirname(file_path))\n",
    "                all_meta.append(current_meta)\n",
    "                last_meta = current_meta\n",
    "                i += 1\n",
    "                continue\n",
    "            if len(fields) >= 2 and all(not is_number(x) for x in fields):\n",
    "                data_header = make_unique(fields)\n",
    "                i += 1\n",
    "                data_rows = []\n",
    "                while i < len(lines):\n",
    "                    row = [x.strip() for x in lines[i].strip().split(',') if x.strip()]\n",
    "                    if len(row) != len(data_header) or not all(is_number(x) for x in row):\n",
    "                        break\n",
    "                    data_rows.append(row)\n",
    "                    i += 1\n",
    "                if data_rows and last_meta:\n",
    "                    temp_df = pd.DataFrame(data_rows, columns=data_header)\n",
    "                    temp_df['SourceFile'] = os.path.basename(file_path)\n",
    "                    temp_df['Folder'] = os.path.basename(os.path.dirname(file_path))\n",
    "                    data_blocks.append((last_meta.copy(), temp_df))\n",
    "                continue\n",
    "            if last_meta and len(fields) >= 2 and all(is_number(x) for x in fields):\n",
    "                data_rows = []\n",
    "                col_count = len(fields)\n",
    "                while i < len(lines):\n",
    "                    row = [x.strip() for x in lines[i].strip().split(',') if x.strip()]\n",
    "                    if len(row) == col_count and all(is_number(x) for x in row):\n",
    "                        data_rows.append(row)\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if data_rows:\n",
    "                    dummy_header = [f\"Col{j+1}\" for j in range(col_count)]\n",
    "                    temp_df = pd.DataFrame(data_rows, columns=dummy_header)\n",
    "                    temp_df['SourceFile'] = os.path.basename(file_path)\n",
    "                    temp_df['Folder'] = os.path.basename(os.path.dirname(file_path))\n",
    "                    data_blocks.append((last_meta.copy(), temp_df))\n",
    "                continue  \n",
    "            i += 1\n",
    "        return data_blocks\n",
    "    except Exception as e:\n",
    "        print(f\"Debug processing failed for {file_path}: {e}\")\n",
    "        return []\n",
    "def log_parser_error_details(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        block_cols = None\n",
    "        block_type = None  # 'meta' or 'data'\n",
    "        block_start = 0\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            raw = line.strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "            cols = [x.strip() for x in raw.split('\\t') if x.strip()] \n",
    "            if not cols:\n",
    "                continue\n",
    "            if all(not is_number(x) for x in cols):\n",
    "                block_cols = len(cols)\n",
    "                block_type = \"header\"\n",
    "                block_start = i + 1\n",
    "                continue\n",
    "            if block_cols is not None and len(cols) != block_cols:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to inspect file: {e}\")\n",
    "def process_csv_with_fallback(file_path):\n",
    "    try:\n",
    "        pairs = process_csv_same_headers_per_file(file_path)\n",
    "        if pairs:\n",
    "            return pairs\n",
    "        print(f\"âš  Format issue detected in {file_path}, attempting recovery with debug mode...\")\n",
    "        log_parser_error_details(file_path) \n",
    "        return debug_process_csv(file_path)\n",
    "    except pd.errors.ParserError as e:\n",
    "        if \"Expected\" in str(e) and \"saw\" in str(e):\n",
    "            log_parser_error_details(file_path)\n",
    "            return debug_process_csv(file_path)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {file_path}: {e}\")\n",
    "        return []\n",
    "def generate_trace_id(row):\n",
    "    fields = ['ship_name', 'ORG_CR_NO', 'ORG_ST_NO', 'latitude', 'LONGITUDE', 'ST_DATE']\n",
    "    parts = [str(row.get(f, \"NA\")).strip() or \"NA\" for f in fields]\n",
    "    return \"_\".join(parts)\n",
    "def folder_has_cnv(folder_path):\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        if any(fname.lower().endswith(\".cnv\") for fname in filenames):\n",
    "            return True\n",
    "    return False\n",
    "def process_csv_folder_recursive(folder_path, output_meta=\"00_Moutput.csv\", output_data=\"00_doutput.csv\"):\n",
    "    all_meta = []\n",
    "    all_data = []\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        if folder_has_cnv(dirpath):\n",
    "            continue\n",
    "        for fname in filenames:\n",
    "            if fname.lower().endswith(\".csv\"):\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                pairs = process_csv_with_fallback(fpath)\n",
    "                for meta_row, data_df in pairs:\n",
    "                    trace_fields = ['ship_name', 'ORG_CR_NO', 'ORG_ST_NO', 'latitude', 'LONGITUDE', 'ST_DATE']\n",
    "                    for f in trace_fields:\n",
    "                        if f not in meta_row:\n",
    "                            meta_row[f] = \"NA\"\n",
    "                    try:\n",
    "                        dt = pd.to_datetime(meta_row['ST_DATE'], errors='coerce')\n",
    "                        meta_row['ST_DATE'] = dt.strftime(\"%b %d %Y %H:%M:%S\") if not pd.isna(dt) else \"NA\"\n",
    "                    except:\n",
    "                        meta_row['ST_DATE'] = \"NA\"\n",
    "                    traceid = generate_trace_id(meta_row)\n",
    "                    meta_row['TraceID'] = traceid\n",
    "                    data_df['TraceID'] = traceid\n",
    "                    data_df['SourceFile'] = os.path.basename(fpath)\n",
    "                    data_df['Folder'] = os.path.basename(os.path.dirname(fpath))\n",
    "                    data_df['folderpath_filename'] = data_df['Folder'] + os.sep + data_df['SourceFile']\n",
    "                    param_cols = [col for col in data_df.columns if col not in {'TraceID', 'SourceFile', 'Folder'}]\n",
    "                    meta_row['Parameters'] = ', '.join(param_cols)\n",
    "                    meta_row['folderpath_filename'] = os.path.join(meta_row['Folder'], meta_row['SourceFile'])\n",
    "\n",
    "                    column_groups = {\n",
    "                        'depSM': ['DepS', 'DepSM', 'Depth', 'Deps', 'depth', 'meters', 'Meters'],\n",
    "                        't090C': ['Temp', 'Temp.(C)', 'T090C', 'T090', 'T068', 'temp'],\n",
    "                        'Sal00': ['Sal00', 'Sal', 'salinity', 'Sal(psu)', 'Salinity'],\n",
    "                    }\n",
    "                    data_df.columns = [col.strip() for col in data_df.columns]\n",
    "                    col_names = list(data_df.columns)\n",
    "                    col_prefixes = [col for col in col_names if re.fullmatch(r'Col\\d+', col)]\n",
    "                    if set(col_prefixes).issubset(col_names):\n",
    "                        col_count = len(col_prefixes)\n",
    "                        col_prefixes_sorted = sorted(col_prefixes, key=lambda x: int(x[3:])) \n",
    "                        rename_map = {}\n",
    "                        if col_count == 2:\n",
    "                            rename_map = dict(zip(col_prefixes_sorted, ['depSM', 't090C']))\n",
    "                        elif col_count == 3:\n",
    "                            rename_map = dict(zip(col_prefixes_sorted, ['depSM', 't090C', 'Sal00']))\n",
    "                        elif col_count == 4:\n",
    "                            if data_df['Col4'].notna().sum() > 0:\n",
    "                                rename_map = dict(zip(col_prefixes_sorted, ['Sequence', 'depSM', 't090C', 'Sal00']))\n",
    "                        if rename_map:\n",
    "                            data_df.rename(columns=rename_map, inplace=True)\n",
    "                    for unified_col, variants in column_groups.items():\n",
    "                        present = [col for col in variants if col in data_df.columns]\n",
    "                        if present:\n",
    "                            data_df[unified_col] = data_df[present].bfill(axis=1).iloc[:, 0]\n",
    "                            to_drop = [col for col in present if col != unified_col]\n",
    "                            data_df.drop(columns=to_drop, inplace=True)\n",
    "                    all_meta.append(pd.DataFrame([meta_row]))\n",
    "                    all_data.append(data_df)\n",
    "    final_meta = pd.concat(all_meta, ignore_index=True) if all_meta else pd.DataFrame()\n",
    "    final_data = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "    if not final_meta.empty:\n",
    "        final_meta.rename(columns={\n",
    "            'latitude': 'Latitude',\n",
    "            'LONGITUDE': 'Longitude',\n",
    "            'ST_DATE': 'Start Time',\n",
    "            'ORG_CR_NO': 'Cruise ID',\n",
    "            'ship_name':'Ship',\n",
    "            'ORG_ST_NO': 'Station'\n",
    "        }, inplace=True)\n",
    "        final_meta.drop(columns=['SourceFile', 'Folder'], errors='ignore', inplace=True)\n",
    "        final_meta.to_csv(output_meta, index=False, na_rep='NaN')\n",
    "        print(f\"Saved metadata to: {output_meta} ({len(final_meta)} rows)\")\n",
    "    else:\n",
    "        print(\"No metadata found\")\n",
    "    if not final_data.empty:\n",
    "        final_data.drop(columns=['SourceFile', 'Folder'], errors='ignore', inplace=True)\n",
    "        final_data.to_csv(output_data, index=False, na_rep='NaN')\n",
    "        print(f\"Saved data to: {output_data} ({len(final_data)} rows)\")\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "folder = r\"C:\\Users\\aishwarya\\OneDrive\\Desktop\\303\\NIO\"\n",
    "process_csv_folder_recursive(folder)\n",
    "#--------QC----------\n",
    "print(\"\\n Running QC on extracted data...\")\n",
    "\n",
    "meta = pd.read_csv('00_Moutput.csv')\n",
    "data = pd.read_csv('00_doutput.csv')\n",
    "raster = rasterio.open(r\"C:\\Users\\aishwarya\\Downloads\\ETOPO1_Bed_g_geotiff\\ETOPO1_Bed_g_geotiff.tif\")\n",
    "bathymetry = raster.read(1)\n",
    "\n",
    "def is_at_sea(lat, lon):\n",
    "    try:\n",
    "        row, col = raster.index(lon, lat)\n",
    "        return bathymetry[row, col] < 0\n",
    "    except:\n",
    "        return False\n",
    "# === Profile Envelope QC Range for TEMP (GTSPP)\n",
    "TEMP_PROFILE_ENVELOPE = [\n",
    "    {\"min_depth\": 0, \"max_depth\": 1100, \"min_value\": -2.0, \"max_value\": 40.0},\n",
    "    {\"min_depth\": 1100, \"max_depth\": 3000, \"min_value\": -1.5, \"max_value\": 18.0},\n",
    "]\n",
    "\n",
    "def get_profile_envelope(depth, envelope_table):\n",
    "    for layer in envelope_table:\n",
    "        if layer[\"min_depth\"] <= depth < layer[\"max_depth\"]:\n",
    "            return layer[\"min_value\"], layer[\"max_value\"]\n",
    "    return None, None\n",
    "def profile_envelope_qc(df, depth_col='depSM', param_col='t090C', envelope_table=TEMP_PROFILE_ENVELOPE):\n",
    "    flags = []\n",
    "    for _, row in df.iterrows():\n",
    "        depth = row.get(depth_col)\n",
    "        value = row.get(param_col)\n",
    "\n",
    "        if pd.isna(depth) or pd.isna(value):\n",
    "            flags.append(9) \n",
    "            continue\n",
    "\n",
    "        min_val, max_val = get_profile_envelope(depth, envelope_table)\n",
    "        if min_val is None:\n",
    "            flags.append(9) \n",
    "        elif min_val <= value <= max_val:\n",
    "            flags.append(1)  \n",
    "        else:\n",
    "            flags.append(4)  \n",
    "    return pd.Series(flags, name=f'{param_col}_PROFILE_QC')\n",
    "\n",
    "# === QC 1: Valid datetime\n",
    "meta['datetime'] = pd.to_datetime(meta['Start Time'], errors='coerce')\n",
    "meta['DATE_QC'] = pd.to_datetime(meta['Start Time'], errors='coerce').dt.year.gt(1997).map({True: 1, False: 4})\n",
    "\n",
    "# === QC 2: Valid position\n",
    "valid_lat = meta['Latitude'].between(-40, 30)\n",
    "valid_lon = meta['Longitude'].between(20, 160)\n",
    "meta['POS_QC'] = ((valid_lat & valid_lon)).map({True: 1, False: 4})\n",
    "\n",
    "# === QC 3: Location at Sea\n",
    "print(\"ðŸ”¹ Checking location at sea...\")\n",
    "meta['SEA_QC'] = meta.apply(lambda row: 1 if is_at_sea(row['Latitude'], row['Longitude']) else 4, axis=1)\n",
    "\n",
    "# === Combine all three station-level QC tests\n",
    "meta_valid = meta[(meta['DATE_QC'] == 1) & (meta['POS_QC'] == 1) & (meta['SEA_QC'] == 1)]\n",
    "\n",
    "# === Filter data to only valid profiles\n",
    "valid_trace_ids = meta_valid['TraceID'].tolist()\n",
    "#data = data[data['TraceID'].isin(valid_trace_ids)]\n",
    "\n",
    "# === Gradient and Spike QC Functions\n",
    "def gradient_test(series, threshold):\n",
    "    result = (series - (series.shift(-1) + series.shift(1)) / 2).abs() <= threshold\n",
    "    return result.map({True: 1, False: 4})\n",
    "\n",
    "def spike_test(series, threshold):\n",
    "    part1 = (series - (series.shift(-1) + series.shift(1)) / 2).abs()\n",
    "    part2 = ((series.shift(-1) - series.shift(1)) / 2).abs()\n",
    "    result = (part1 - part2) <= threshold\n",
    "    return result.map({True: 1, False: 4})\n",
    "\n",
    "# === QC 4â€“6: Variable-level QC for TEMP and PSAL\n",
    "if 't090C' in data.columns:\n",
    "    data['TEMP_QC'] = data['t090C'].between(-2, 40).map({True: 1, False: 4})\n",
    "    data['TEMP_GRAD_QC'] = gradient_test(data['t090C'], 10.0)\n",
    "    data['TEMP_SPIKE_QC'] = spike_test(data['t090C'], 2.0)\n",
    "\n",
    "if 'Sal00' in data.columns:\n",
    "    data['PSAL_QC'] = data['Sal00'].between(0, 41).map({True: 1, False: 4})\n",
    "    data['PSAL_GRAD_QC'] = gradient_test(data['Sal00'], 5.0)\n",
    "    data['PSAL_SPIKE_QC'] = spike_test(data['Sal00'], 0.3)\n",
    "# === QC 7: Profile Envelope Test\n",
    "if 'depSM' in data.columns and 't090C' in data.columns:\n",
    "    data['TEMP_PROFILE_QC'] = profile_envelope_qc(data, depth_col='depSM', param_col='t090C')\n",
    "\n",
    "#meta.drop(columns=['at_sea'], inplace=True)\n",
    "meta.to_csv(\"meta9.csv\", index=False,na_rep='NaN')\n",
    "data.to_csv(\"data9.csv\", index=False,na_rep='NaN')\n",
    "\n",
    "print(\" QC complete. Saved meta9.csv and data9.csv \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8486d-f339-4a82-afcc-ffb9f43af1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
